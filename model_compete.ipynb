{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197919c9",
   "metadata": {},
   "source": [
    "### Training PVP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f482dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selfplay_train.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from makruk_env import FairyStockfishMakruk\n",
    "\n",
    "class SelfPlayMakruk(FairyStockfishMakruk):\n",
    "    \"\"\"\n",
    "    A Makruk Env where the 'opponent' is a fixed PPO model.\n",
    "    We update self (the 'current' PPO) but keep self.opponent frozen.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        opponent_path: str,\n",
    "        device: str = \"cpu\",\n",
    "        **kwargs  # everything FairyStockfishMakruk.__init__ accepts\n",
    "    ):\n",
    "        # 1) Init base env\n",
    "        super().__init__(**kwargs)\n",
    "        # 2) Load frozen opponent\n",
    "        self.opponent: MaskablePPO = MaskablePPO.load(opponent_path, device=device)\n",
    "        # 3) Force self-play mode\n",
    "        self.play_mode = \"selfplay\"\n",
    "\n",
    "    def step(self, action):\n",
    "        # 1) Agent plays\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        if done:\n",
    "            return obs, reward, done, truncated, info\n",
    "\n",
    "        # 2) Opponent plays\n",
    "        mask = self.get_legal_moves_mask()\n",
    "        opp_act, _ = self.opponent.predict(obs, action_masks=mask, deterministic=True)\n",
    "        obs, opp_reward, done, truncated, info = super().step(opp_act)\n",
    "\n",
    "        # 3) Invert opponent’s reward\n",
    "        return obs, reward - opp_reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794d4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ————————————————————————————————————————————————\n",
    "\n",
    "# 1) Paths to your checkpoints\n",
    "CURRENT_MODEL_PATH = \"./ppo_makruk_pvp.zip\"\n",
    "# CURRENT_MODEL_PATH = \"./ppo_makruk_self_pvp.zip\"\n",
    "# CURRENT_MODEL_PATH = \"./ppo_makruk_notebook.zip\"\n",
    "# BEST_MODEL_PATH    = \"./ppo_makruk_pvp.zip\"\n",
    "BEST_MODEL_PATH    = \"./ppo_makruk_self_pvp.zip\"\n",
    "# BEST_MODEL_PATH    = \"./best_model/best_model.zip\"\n",
    "\n",
    "# 2) Build a vectorized self-play env\n",
    "def make_env(rank):\n",
    "    def _init():\n",
    "        env = SelfPlayMakruk(\n",
    "            opponent_path=BEST_MODEL_PATH,\n",
    "            device=\"mps\",                   # only used by SelfPlayMakruk to load its opponent\n",
    "            path=\"./engine/fairy-stockfish-arm\",\n",
    "            max_depth=3,                    # opponent difficulty\n",
    "            engine_timeout=2.0,\n",
    "            render_mode=None                # headless\n",
    "        )\n",
    "        # SB3 logging wrapper\n",
    "        env = Monitor(env)\n",
    "        # Masking wrapper — unwrap the Monitor so get_legal_moves_mask() is on the right object\n",
    "        env = ActionMasker(env, lambda m: m.env.get_legal_moves_mask())\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c033c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from ./ppo_makruk_pvp.zip\n",
      "Logging to ./ppo_selfplay_tb/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harryphoebus/miniconda3/envs/deep_rl/lib/python3.11/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_envs = 8\n",
    "vec = DummyVecEnv([make_env(i) for i in range(n_envs)])\n",
    "env = VecMonitor(vec)\n",
    "\n",
    "# ————————————————————————————————————————————————\n",
    "\n",
    "# 3) Load or create your “current” PPO\n",
    "if os.path.isfile(CURRENT_MODEL_PATH):\n",
    "    \n",
    "    print(f\"Resuming from {CURRENT_MODEL_PATH}\")\n",
    "    model = MaskablePPO.load(CURRENT_MODEL_PATH, env=env, device=\"mps\")\n",
    "\n",
    "    # avoid clobbering old logs\n",
    "    new_logger = configure(\"./ppo_selfplay_tb/\", [\"stdout\", \"tensorboard\"])\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    # —––––––––––––––––––––––––––––––––––––––––––––––––––—\n",
    "    # **Tweak hyperparams for self-play**  \n",
    "    # keep exploring\n",
    "    model.ent_coef = 1e-2\n",
    "    # lower LR so it absorbs that exploration\n",
    "    for pg in model.policy.optimizer.param_groups:\n",
    "        pg[\"lr\"] = 1e-4\n",
    "else:\n",
    "    print(\"Starting fresh model\")\n",
    "    model = MaskablePPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        device=\"mps\",\n",
    "        verbose=1,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        learning_rate=2.5e-4,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=1e-2,\n",
    "        tensorboard_log=\"./ppo_selfplay_tb/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b2e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.2     |\n",
      "|    ep_rew_mean     | 0.508    |\n",
      "| time/              |          |\n",
      "|    fps             | 96       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 32.9        |\n",
      "|    ep_rew_mean          | 0.519       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 373         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008909529 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.761      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0861      |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35          |\n",
      "|    ep_rew_mean          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008105129 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.783      |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.1        |\n",
      "|    ep_rew_mean          | 0.488       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 778         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007819295 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.732      |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35.4        |\n",
      "|    ep_rew_mean          | 0.515       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 977         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007671383 |\n",
      "|    clip_fraction        | 0.0835      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.728      |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0667      |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37.5         |\n",
      "|    ep_rew_mean          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 1172         |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075399103 |\n",
      "|    clip_fraction        | 0.0833       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0737       |\n",
      "|    n_updates            | 3420         |\n",
      "|    policy_gradient_loss | -0.0238      |\n",
      "|    value_loss           | 0.203        |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 38.4         |\n",
      "|    ep_rew_mean          | 0.457        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 1366         |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077625993 |\n",
      "|    clip_fraction        | 0.0815       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.698       |\n",
      "|    explained_variance   | 0.127        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.096        |\n",
      "|    n_updates            | 3430         |\n",
      "|    policy_gradient_loss | -0.0221      |\n",
      "|    value_loss           | 0.181        |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.5         |\n",
      "|    ep_rew_mean          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 1575         |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077156625 |\n",
      "|    clip_fraction        | 0.0828       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.697       |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0397       |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.0223      |\n",
      "|    value_loss           | 0.192        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.1         |\n",
      "|    ep_rew_mean          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 1775         |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076490343 |\n",
      "|    clip_fraction        | 0.0804       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.653       |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 3450         |\n",
      "|    policy_gradient_loss | -0.0225      |\n",
      "|    value_loss           | 0.21         |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 34.8       |\n",
      "|    ep_rew_mean          | 0.549      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 1974       |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00733565 |\n",
      "|    clip_fraction        | 0.0798     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.68      |\n",
      "|    explained_variance   | 0.196      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0534     |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    value_loss           | 0.203      |\n",
      "----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 22 moves\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.2        |\n",
      "|    ep_rew_mean          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 2183        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006936974 |\n",
      "|    clip_fraction        | 0.0758      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.04        |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 34.5         |\n",
      "|    ep_rew_mean          | 0.525        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 2391         |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075266324 |\n",
      "|    clip_fraction        | 0.0777       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.641       |\n",
      "|    explained_variance   | 0.201        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0449       |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -0.0214      |\n",
      "|    value_loss           | 0.198        |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.8        |\n",
      "|    ep_rew_mean          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 2598        |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007091714 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0399      |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 40           |\n",
      "|    ep_rew_mean          | 0.903        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 2807         |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075031146 |\n",
      "|    clip_fraction        | 0.0823       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.651       |\n",
      "|    explained_variance   | 0.134        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0461       |\n",
      "|    n_updates            | 3500         |\n",
      "|    policy_gradient_loss | -0.0226      |\n",
      "|    value_loss           | 0.22         |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 39           |\n",
      "|    ep_rew_mean          | 0.776        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 3016         |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072746594 |\n",
      "|    clip_fraction        | 0.0774       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.163        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0512       |\n",
      "|    n_updates            | 3510         |\n",
      "|    policy_gradient_loss | -0.021       |\n",
      "|    value_loss           | 0.162        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 40.6         |\n",
      "|    ep_rew_mean          | 0.815        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 3226         |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068945736 |\n",
      "|    clip_fraction        | 0.0767       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.58        |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.082        |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.0208      |\n",
      "|    value_loss           | 0.187        |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37.4         |\n",
      "|    ep_rew_mean          | 0.713        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 3440         |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065157013 |\n",
      "|    clip_fraction        | 0.0676       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.55        |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.126        |\n",
      "|    n_updates            | 3530         |\n",
      "|    policy_gradient_loss | -0.02        |\n",
      "|    value_loss           | 0.183        |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37.7         |\n",
      "|    ep_rew_mean          | 0.727        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 3641         |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070392885 |\n",
      "|    clip_fraction        | 0.0741       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.559       |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0437       |\n",
      "|    n_updates            | 3540         |\n",
      "|    policy_gradient_loss | -0.0209      |\n",
      "|    value_loss           | 0.197        |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.1         |\n",
      "|    ep_rew_mean          | 0.72         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 3837         |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075764614 |\n",
      "|    clip_fraction        | 0.0769       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.154        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0719       |\n",
      "|    n_updates            | 3550         |\n",
      "|    policy_gradient_loss | -0.0216      |\n",
      "|    value_loss           | 0.197        |\n",
      "------------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38.9        |\n",
      "|    ep_rew_mean          | 0.748       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 4033        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006986019 |\n",
      "|    clip_fraction        | 0.071       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 33.6        |\n",
      "|    ep_rew_mean          | 0.897       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 4236        |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006695596 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 16 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.7        |\n",
      "|    ep_rew_mean          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 4438        |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007203386 |\n",
      "|    clip_fraction        | 0.0735      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0775      |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37          |\n",
      "|    ep_rew_mean          | 0.866       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 4640        |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006752288 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.52       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "DRAW TRIGGERED: counting rule draw after 8 moves\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.4        |\n",
      "|    ep_rew_mean          | 0.862       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 4845        |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006781698 |\n",
      "|    clip_fraction        | 0.0691      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0784      |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37.1         |\n",
      "|    ep_rew_mean          | 0.777        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 5043         |\n",
      "|    total_timesteps      | 409600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070996755 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.565       |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0555       |\n",
      "|    n_updates            | 3610         |\n",
      "|    policy_gradient_loss | -0.0208      |\n",
      "|    value_loss           | 0.202        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ————————————————————————————————————————————————\n",
    "\n",
    "# 4) Callbacks\n",
    "class BestSaver(BaseCallback):\n",
    "    def __init__(self, save_freq: int, best_path: str, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.save_freq = save_freq\n",
    "        self.best_path = best_path\n",
    "        self.best_mean_reward = -float(\"inf\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            mean_r = self.model.logger.name_to_value.get(\"rollout/ep_rew_mean\")\n",
    "            if mean_r is not None and mean_r > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_r\n",
    "                self.model.save(self.best_path)\n",
    "                print(f\"[BestSaver] New best {mean_r:.2f} → {self.best_path}\")\n",
    "        return True\n",
    "\n",
    "checkpoint_cb = CheckpointCallback(\n",
    "    save_freq=100_000, save_path=\"./checkpoints/\", name_prefix=\"selfplay\"\n",
    ")\n",
    "eval_cb = MaskableEvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=\"./eval_best/\",\n",
    "    log_path=\"./eval_logs/\",\n",
    "    eval_freq=100_000,\n",
    "    n_eval_episodes=16,\n",
    "    deterministic=True\n",
    ")\n",
    "best_cb = BestSaver(save_freq=50_000, best_path=BEST_MODEL_PATH)\n",
    "\n",
    "# ————————————————————————————————————————————————\n",
    "\n",
    "# 5) Train\n",
    "model.learn(\n",
    "    total_timesteps=400_000,\n",
    "    callback=[checkpoint_cb, eval_cb, best_cb]\n",
    ")\n",
    "\n",
    "# 6) Save your “current” policy\n",
    "model.save(\"ppo_makruk_pvp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fed731",
   "metadata": {},
   "source": [
    "### Evaluation against Own\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c14481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Paths to your checkpoints\n",
    "# CURRENT_MODEL_PATH = \"./ppo_makruk_pvp.zip\"\n",
    "CURRENT_MODEL_PATH = \"./ppo_makruk_self_pvp.zip\"\n",
    "# CURRENT_MODEL_PATH = \"./ppo_makruk_notebook.zip\"\n",
    "# BEST_MODEL_PATH    = \"./ppo_makruk_pvp.zip\"\n",
    "BEST_MODEL_PATH    = \"./best_model/best_model.zip\"\n",
    "# BEST_MODEL_PATH    = \"./ppo_makruk_self_pvp.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6269f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import MaskablePPO\n",
    "model = MaskablePPO.load(CURRENT_MODEL_PATH, device=\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6725b717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Against frozen best over 100 games → wins=60, losses=37\n"
     ]
    }
   ],
   "source": [
    "# 7) Quick headless self-play evaluation vs. the frozen “best” model\n",
    "eval_env = SelfPlayMakruk(\n",
    "    opponent_path=BEST_MODEL_PATH,\n",
    "    device=\"mps\",\n",
    "    path=\"./engine/fairy-stockfish-arm\",\n",
    "    max_depth=2,            # opponent difficulty\n",
    "    engine_timeout=2.0,\n",
    "    render_mode=None        # headless\n",
    ")\n",
    "\n",
    "# 3) Run N episodes, tallying wins vs. losses\n",
    "n_eval = 100\n",
    "wins, losses = 0, 0\n",
    "\n",
    "for _ in range(n_eval):\n",
    "    obs, info = eval_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        mask = eval_env.get_legal_moves_mask()\n",
    "        action, _ = model.predict(obs, action_masks=mask, deterministic=True)\n",
    "        obs, reward, done, _, info = eval_env.step(action)\n",
    "\n",
    "    # reward > 0  → our agent delivered mate (win)\n",
    "    # reward < 0  → opponent delivered mate (loss)\n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "    elif reward < 0:\n",
    "        losses += 1\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Against frozen best over {n_eval} games → wins={wins}, losses={losses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb82f3",
   "metadata": {},
   "source": [
    "### Evaluation Against Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "653e4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Paths to your checkpoints\n",
    "CURRENT_MODEL_PATH = \"./ppo_makruk_pvp.zip\"\n",
    "# CURRENT_MODEL_PATH = \"./ppo_makruk_self_pvp.zip\"\n",
    "# CURRENT_MODEL_PATH = \"./best_model/best_model.zip\"\n",
    "# CURRENT_MODEL_PATH = \"./ppo_makruk_notebook.zip\"\n",
    "BEST_MODEL_PATH    = \"./ppo_makruk_pvp.zip\"\n",
    "# BEST_MODEL_PATH    = \"./best_model/best_model.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba33f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import MaskablePPO\n",
    "model = MaskablePPO.load(CURRENT_MODEL_PATH, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f50836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 100 games at depth=1 → wins=0, losses=56\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sb3_contrib import MaskablePPO\n",
    "from makruk_env import FairyStockfishMakruk\n",
    "\n",
    "\n",
    "\n",
    "# 2) Build a fresh headless env at your chosen difficulty\n",
    "eval_env = FairyStockfishMakruk(\n",
    "    path=\"./engine/fairy-stockfish-arm\",\n",
    "    max_depth=2,            # ← set difficulty here\n",
    "    play_mode=\"selfplay\",\n",
    "    engine_timeout=2.0,\n",
    "    render_mode=None        # headless\n",
    ")\n",
    "\n",
    "# 3) Run N games, counting only wins vs. losses\n",
    "n_eval = 100\n",
    "wins = 0\n",
    "losses = 0\n",
    "\n",
    "for _ in range(n_eval):\n",
    "    obs, info = eval_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        mask = eval_env.get_legal_moves_mask()\n",
    "        action, _ = model.predict(obs, action_masks=mask, deterministic=True)\n",
    "        obs, reward, done, _, info = eval_env.step(action)\n",
    "\n",
    "    # +10 → agent mate → win; –10 → engine mate → loss; 0 (draw) ignored\n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "    elif reward < 0:\n",
    "        losses += 1\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Over {n_eval} games at depth=1 → wins={wins}, losses={losses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59725492",
   "metadata": {},
   "source": [
    "### GUI Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 14:15:55.692 python[96095:28355733] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-05-07 14:15:55.692 python[96095:28355733] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over: checkmate, reward=-1.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %% Testing cell (GUI only, with explicit mask)\n",
    "\n",
    "import time\n",
    "import pygame\n",
    "from sb3_contrib import MaskablePPO\n",
    "from makruk_env import FairyStockfishMakruk\n",
    "\n",
    "# 1) Load your trained model\n",
    "model = MaskablePPO.load(CURRENT_MODEL_PATH, device=\"mps\")\n",
    "# model = MaskablePPO.load(\"./best_model/best_model.zip\", device=\"mps\")\n",
    "\n",
    "# 2) Create the GUI env\n",
    "env = FairyStockfishMakruk(\n",
    "    path=\"./engine/fairy-stockfish-arm\",\n",
    "    max_depth=2,\n",
    "    play_mode=\"selfplay\",\n",
    "    engine_timeout=2.0,\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "# 3) Reset & initial draw\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "time.sleep(1.0)\n",
    "\n",
    "# 4) Play one self-play game\n",
    "done = False\n",
    "while not done:\n",
    "    # Keep window alive\n",
    "    for e in pygame.event.get():\n",
    "        if e.type == pygame.QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    # 4a) Compute the mask of legal moves\n",
    "    mask = env.get_legal_moves_mask()\n",
    "\n",
    "    # 4b) Tell the model to only pick from those legal moves  \n",
    "    #     Note: the keyword is **action_masks=**, not mask=\n",
    "    action, _ = model.predict(obs, action_masks=mask, deterministic=True)\n",
    "    \n",
    "    # 5) Step and render\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# 6) Done\n",
    "print(f\"Game over: {info.get('end_reason')}, reward={reward}\")\n",
    "env.close()\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f130277",
   "metadata": {},
   "source": [
    "### Tensor Board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795c500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6010 (pid 33929), started 0:00:01 ago. (Use '!kill 33929' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9d5fdb1f6a381355\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9d5fdb1f6a381355\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# in the very first cell\n",
    "%load_ext tensorboard\n",
    "\n",
    "# in any cell\n",
    "%tensorboard --logdir ./ppo_selfplay_tb --port 6010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628cd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import MaskablePPO\n",
    "import torch\n",
    "# load your MPS-trained model\n",
    "model = MaskablePPO.load(\"./FInalModel/ppo_makruk_pvp.zip\", env=env, device=\"mps\")\n",
    "\n",
    "# move everything to CPU\n",
    "model.policy.to(\"cpu\")\n",
    "model.device = torch.device(\"cpu\")\n",
    "\n",
    "# overwrite the file (or save to a new name)\n",
    "model.save(\"ppo_makruk_cpu.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
