{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Training cell (with GUI callback that disables itself on window‐close)\n",
    "\n",
    "# Toggle GUI vs headless\n",
    "USE_GUI = False        # ← True to see Pygame GUI during training\n",
    "TOTAL_STEPS = 300000  # adjust for your run\n",
    "ENGINE_DEPTH= 1\n",
    "\n",
    "import os\n",
    "import time\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from makruk_env import FairyStockfishMakruk\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "\n",
    "class RenderCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Renders the GUI each step; if the window is closed, disables further rendering.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.enabled = True\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if not self.enabled:\n",
    "            return True\n",
    "\n",
    "        # 1) Unwrap to the base FairyStockfishMakruk env\n",
    "        env_obj = self.model.env\n",
    "        if hasattr(env_obj, \"envs\"):\n",
    "            env_obj = env_obj.envs[0]\n",
    "        while hasattr(env_obj, \"env\"):\n",
    "            env_obj = env_obj.env\n",
    "\n",
    "        # 2) Try rendering; if it fails, disable future renders\n",
    "        try:\n",
    "            env_obj.render()\n",
    "        except Exception:\n",
    "            print(\"RenderCallback: GUI closed; disabling further rendering.\")\n",
    "            self.enabled = False\n",
    "            return True\n",
    "\n",
    "        # 3) On terminal, pause so you can see the final position\n",
    "        if getattr(env_obj, \"done\", False):\n",
    "            reason = env_obj.info.get(\"end_reason\", \"?\")\n",
    "            print(f\"\\n→ Episode ended: {reason}\\n\")\n",
    "            time.sleep(1.0)\n",
    "\n",
    "        return True\n",
    "\n",
    "def make_gui_env():\n",
    "    e = FairyStockfishMakruk(\n",
    "        path=\"./engine/fairy-stockfish-arm\",\n",
    "        max_depth= ENGINE_DEPTH,\n",
    "        play_mode=\"selfplay\",\n",
    "        engine_timeout=2.0,\n",
    "        render_mode=\"human\"\n",
    "    )\n",
    "    e = Monitor(e)\n",
    "    e = ActionMasker(e, lambda ev: ev.env.get_legal_moves_mask())\n",
    "    return e\n",
    "\n",
    "def make_headless_env(rank):\n",
    "    e = FairyStockfishMakruk(\n",
    "        path=\"./engine/fairy-stockfish-arm\",\n",
    "        max_depth= ENGINE_DEPTH,\n",
    "        play_mode=\"selfplay\",\n",
    "        engine_timeout=2.0,\n",
    "        render_mode=None\n",
    "    )\n",
    "    e = Monitor(e)\n",
    "    e = ActionMasker(e, lambda ev: ev.env.get_legal_moves_mask())\n",
    "    return e\n",
    "\n",
    "# — Build environment & callbacks —\n",
    "if USE_GUI:\n",
    "    print(\">>> GUI training mode\")\n",
    "    env = make_gui_env()\n",
    "    extra_cbs = [RenderCallback()]\n",
    "else:\n",
    "    print(\">>> Headless vectorized training\")\n",
    "    n_envs = 8\n",
    "    vec = DummyVecEnv([lambda i=i: make_headless_env(i) for i in range(n_envs)])\n",
    "    env = VecMonitor(vec)\n",
    "    extra_cbs = []\n",
    "\n",
    "checkpoint_cb = CheckpointCallback(\n",
    "    save_freq=50_000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_makruk\"\n",
    ")\n",
    "eval_cb = MaskableEvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    log_path=\"./eval_logs/\",\n",
    "    eval_freq=50_000,\n",
    "    n_eval_episodes=8,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# — Load existing model or create new —\n",
    "# model_path = \"./checkpoints/ppo_makruk_400000_steps.zip\"\n",
    "# model_path = \"./best_model/best_model.zip\"\n",
    "# model_path = \"ppo_makruk_notebook.zip\"\n",
    "# model_path = \"ppo_makruk_pvp.zip\"\n",
    "model_path = \"ppo_makruk_self_pvp.zip\"\n",
    "\n",
    "if os.path.isfile(model_path):\n",
    "    print(f\"Resuming from {model_path}\")\n",
    "    # 1) Load the checkpoint into your existing env\n",
    "    model = MaskablePPO.load(model_path, env=env, device=\"mps\")\n",
    "\n",
    "    # 2) Tweak hyperparameters in place:\n",
    "    # — add some entropy bonus so the policy keeps exploring\n",
    "    model.ent_coef =  1e-2   #  5e-2\n",
    "\n",
    "    # — lower the learning rate to 1e-4\n",
    "    for pg in model.policy.optimizer.param_groups:\n",
    "        pg[\"lr\"] = 1e-4\n",
    "\n",
    "else:\n",
    "    print(\"Starting fresh model\")\n",
    "    model = MaskablePPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        device=\"mps\",\n",
    "        verbose=1,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        learning_rate=2.5e-4,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.0,           # start with no entropy bonus if you like\n",
    "        tensorboard_log=\"./ppo_makruk_tb/\"\n",
    "    )\n",
    "\n",
    "# if os.path.isfile(model_path):\n",
    "#     print(f\"Resuming from {model_path}\")\n",
    "#     model = MaskablePPO.load(model_path, env=env, device=\"mps\")\n",
    "# else:\n",
    "#     print(\"Starting fresh model\")\n",
    "#     model = MaskablePPO(\n",
    "#         policy=\"MlpPolicy\",\n",
    "#         env=env,\n",
    "#         device=\"mps\",\n",
    "#         verbose=1,\n",
    "#         n_steps=2048,\n",
    "#         batch_size=64,\n",
    "#         gamma=0.99,\n",
    "#         gae_lambda=0.95,\n",
    "#         learning_rate=2.5e-4,\n",
    "#         clip_range=0.2,\n",
    "#         tensorboard_log=\"./ppo_makruk_tb/\"\n",
    "#     )\n",
    "\n",
    "# — Train —\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    callback=[checkpoint_cb, eval_cb] + extra_cbs\n",
    ")\n",
    "\n",
    "# model.learn(\n",
    "#     total_timesteps=TOTAL_STEPS,\n",
    "#     reset_num_timesteps=False,  # keep counting from your checkpoint\n",
    "#     callback=[checkpoint_cb, eval_cb] + extra_cbs\n",
    "# )\n",
    "\n",
    "# — Save —\n",
    "model.save(\"ppo_makruk_self_pvp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2bd77",
   "metadata": {},
   "source": [
    "# Bruh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d0d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Headless vectorized training\n",
      "Resuming from ppo_makruk_self_pvp.zip\n",
      "Logging to ./ppo_makruk_tb/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harryphoebus/miniconda3/envs/deep_rl/lib/python3.11/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32.1     |\n",
      "|    ep_rew_mean     | -9.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 251      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35.1        |\n",
      "|    ep_rew_mean          | -9.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011772556 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | -0.0981     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.167       |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    value_loss           | 0.931       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 53.9        |\n",
      "|    ep_rew_mean          | -8.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008972377 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.041       |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 0.443       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 97.6       |\n",
      "|    ep_rew_mean          | -8.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 306        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00401474 |\n",
      "|    clip_fraction        | 0.0461     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.332     |\n",
      "|    explained_variance   | 0.752      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00413   |\n",
      "|    n_updates            | 2250       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    value_loss           | 0.318      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98.2         |\n",
      "|    ep_rew_mean          | -9.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 220          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 371          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032161083 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0237      |\n",
      "|    value_loss           | 0.206        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.2        |\n",
      "|    ep_rew_mean          | -9.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002587107 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.131      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.0503      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 231      |\n",
      "|    iterations           | 7        |\n",
      "|    time_elapsed         | 495      |\n",
      "|    total_timesteps      | 114688   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.00219  |\n",
      "|    n_updates            | 2280     |\n",
      "|    policy_gradient_loss | 3.98e-09 |\n",
      "|    value_loss           | 0.00371  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 235      |\n",
      "|    iterations           | 8        |\n",
      "|    time_elapsed         | 556      |\n",
      "|    total_timesteps      | 131072   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 5.64e-05 |\n",
      "|    n_updates            | 2290     |\n",
      "|    policy_gradient_loss | 9.67e-09 |\n",
      "|    value_loss           | 0.00261  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 238      |\n",
      "|    iterations           | 9        |\n",
      "|    time_elapsed         | 619      |\n",
      "|    total_timesteps      | 147456   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.000722 |\n",
      "|    n_updates            | 2300     |\n",
      "|    policy_gradient_loss | 2.32e-08 |\n",
      "|    value_loss           | 0.00171  |\n",
      "--------------------------------------\n",
      "[EntropyAnnealer] Switching ent_coef → 0.001\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 98.2      |\n",
      "|    ep_rew_mean          | -9.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 242       |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 674       |\n",
      "|    total_timesteps      | 163840    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0         |\n",
      "|    explained_variance   | 0.959     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 4.5e-05   |\n",
      "|    n_updates            | 2310      |\n",
      "|    policy_gradient_loss | -1.43e-09 |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 244      |\n",
      "|    iterations           | 11       |\n",
      "|    time_elapsed         | 735      |\n",
      "|    total_timesteps      | 180224   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.00172  |\n",
      "|    n_updates            | 2320     |\n",
      "|    policy_gradient_loss | 2.38e-09 |\n",
      "|    value_loss           | 0.000834 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 246      |\n",
      "|    iterations           | 12       |\n",
      "|    time_elapsed         | 798      |\n",
      "|    total_timesteps      | 196608   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.000176 |\n",
      "|    n_updates            | 2330     |\n",
      "|    policy_gradient_loss | 1.6e-08  |\n",
      "|    value_loss           | 0.000649 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 248      |\n",
      "|    iterations           | 13       |\n",
      "|    time_elapsed         | 858      |\n",
      "|    total_timesteps      | 212992   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.000677 |\n",
      "|    n_updates            | 2340     |\n",
      "|    policy_gradient_loss | -3.8e-09 |\n",
      "|    value_loss           | 0.000504 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 249      |\n",
      "|    iterations           | 14       |\n",
      "|    time_elapsed         | 919      |\n",
      "|    total_timesteps      | 229376   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 1.38e-05 |\n",
      "|    n_updates            | 2350     |\n",
      "|    policy_gradient_loss | 8.67e-09 |\n",
      "|    value_loss           | 0.000327 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 250      |\n",
      "|    iterations           | 15       |\n",
      "|    time_elapsed         | 982      |\n",
      "|    total_timesteps      | 245760   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 2.84e-05 |\n",
      "|    n_updates            | 2360     |\n",
      "|    policy_gradient_loss | -9.6e-10 |\n",
      "|    value_loss           | 0.000253 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 250      |\n",
      "|    iterations           | 16       |\n",
      "|    time_elapsed         | 1047     |\n",
      "|    total_timesteps      | 262144   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.000162 |\n",
      "|    n_updates            | 2370     |\n",
      "|    policy_gradient_loss | 1.22e-08 |\n",
      "|    value_loss           | 0.000169 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 98.2      |\n",
      "|    ep_rew_mean          | -9.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 251       |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 1109      |\n",
      "|    total_timesteps      | 278528    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0         |\n",
      "|    explained_variance   | 0.959     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 7.99e-06  |\n",
      "|    n_updates            | 2380      |\n",
      "|    policy_gradient_loss | -3.21e-09 |\n",
      "|    value_loss           | 0.000118  |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 250      |\n",
      "|    iterations           | 18       |\n",
      "|    time_elapsed         | 1175     |\n",
      "|    total_timesteps      | 294912   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 1.09e-05 |\n",
      "|    n_updates            | 2390     |\n",
      "|    policy_gradient_loss | 1.61e-08 |\n",
      "|    value_loss           | 0.000101 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 98.2     |\n",
      "|    ep_rew_mean          | -9.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 251      |\n",
      "|    iterations           | 19       |\n",
      "|    time_elapsed         | 1240     |\n",
      "|    total_timesteps      | 311296   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0        |\n",
      "|    explained_variance   | 0.959    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 5.85e-06 |\n",
      "|    n_updates            | 2400     |\n",
      "|    policy_gradient_loss | 1.65e-09 |\n",
      "|    value_loss           | 9.09e-05 |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# %% Training cell (with GUI callback that disables itself on window‐close)\n",
    "\n",
    "# Toggle GUI vs headless\n",
    "USE_GUI = False        # ← True to see Pygame GUI during training\n",
    "TOTAL_STEPS = 300_000  # adjust for your run\n",
    "ENGINE_DEPTH = 1\n",
    "\n",
    "import os\n",
    "import time\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from makruk_env import FairyStockfishMakruk\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Prepare a linear LR schedule\n",
    "lr_schedule = get_schedule_fn(1e-4)  # start at 1e-4, linearly decay to 0 over TOTAL_STEPS\n",
    "\n",
    "# 2) Callback to anneal entropy halfway\n",
    "class EntropyAnnealer(BaseCallback):\n",
    "    def __init__(self, switch_step: int, new_ent: float, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.switch_step = switch_step\n",
    "        self.new_ent = new_ent\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # once we hit halfway, lower entropy coefficient\n",
    "        if self.num_timesteps == self.switch_step:\n",
    "            print(f\"[EntropyAnnealer] Switching ent_coef → {self.new_ent}\")\n",
    "            self.model.ent_coef = self.new_ent\n",
    "        return True\n",
    "\n",
    "entropy_cb = EntropyAnnealer(switch_step=TOTAL_STEPS // 2, new_ent=1e-3)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class RenderCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Renders the GUI each step; if the window is closed, disables further rendering.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.enabled = True\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if not self.enabled:\n",
    "            return True\n",
    "        # unwrap to the base env and render\n",
    "        env_obj = self.model.env\n",
    "        if hasattr(env_obj, \"envs\"):\n",
    "            env_obj = env_obj.envs[0]\n",
    "        while hasattr(env_obj, \"env\"):\n",
    "            env_obj = env_obj.env\n",
    "        try:\n",
    "            env_obj.render()\n",
    "        except Exception:\n",
    "            print(\"RenderCallback: GUI closed; disabling further rendering.\")\n",
    "            self.enabled = False\n",
    "        return True\n",
    "\n",
    "def make_gui_env():\n",
    "    e = FairyStockfishMakruk(\n",
    "        path=\"./engine/fairy-stockfish-arm\",\n",
    "        max_depth=ENGINE_DEPTH,\n",
    "        play_mode=\"selfplay\",\n",
    "        engine_timeout=2.0,\n",
    "        render_mode=\"human\"\n",
    "    )\n",
    "    e = Monitor(e)\n",
    "    e = ActionMasker(e, lambda ev: ev.env.get_legal_moves_mask())\n",
    "    return e\n",
    "\n",
    "def make_headless_env(rank):\n",
    "    e = FairyStockfishMakruk(\n",
    "        path=\"./engine/fairy-stockfish-arm\",\n",
    "        max_depth=ENGINE_DEPTH,\n",
    "        play_mode=\"selfplay\",\n",
    "        engine_timeout=2.0,\n",
    "        render_mode=None\n",
    "    )\n",
    "    e = Monitor(e)\n",
    "    e = ActionMasker(e, lambda ev: ev.env.get_legal_moves_mask())\n",
    "    return e\n",
    "\n",
    "# — Build environment & callbacks —\n",
    "if USE_GUI:\n",
    "    print(\">>> GUI training mode\")\n",
    "    env = make_gui_env()\n",
    "    extra_cbs = [RenderCallback()]\n",
    "else:\n",
    "    print(\">>> Headless vectorized training\")\n",
    "    n_envs = 8\n",
    "    vec = DummyVecEnv([lambda i=i: make_headless_env(i) for i in range(n_envs)])\n",
    "    env = VecMonitor(vec)\n",
    "    extra_cbs = []\n",
    "\n",
    "checkpoint_cb = CheckpointCallback(\n",
    "    save_freq=50_000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_makruk\"\n",
    ")\n",
    "eval_cb = MaskableEvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    log_path=\"./eval_logs/\",\n",
    "    eval_freq=50_000,\n",
    "    n_eval_episodes=8,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# — Load existing model or create new —\n",
    "# model_path = \"./best_model/best_model.zip\"\n",
    "# model_path = \"ppo_makruk_notebook.zip\"\n",
    "# model_path = \"ppo_makruk_pvp.zip\"\n",
    "model_path = \"ppo_makruk_self_pvp.zip\"\n",
    "\n",
    "if os.path.isfile(model_path):\n",
    "    print(f\"Resuming from {model_path}\")\n",
    "    model = MaskablePPO.load(model_path, env=env, device=\"mps\")\n",
    "    # override ent_coef & install new LR schedule\n",
    "    model.ent_coef = 5e-2\n",
    "    model.lr_schedule = lr_schedule\n",
    "    # step optimizer to current lr\n",
    "    for pg in model.policy.optimizer.param_groups:\n",
    "        pg[\"lr\"] = lr_schedule(model.num_timesteps)\n",
    "    # re-configure logger so self-play logs go to a fresh folder\n",
    "    new_logger = configure(\"./ppo_makruk_tb/\", [\"stdout\", \"tensorboard\"])\n",
    "    model.set_logger(new_logger)\n",
    "else:\n",
    "    print(\"Starting fresh model\")\n",
    "    model = MaskablePPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        device=\"mps\",\n",
    "        verbose=1,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        learning_rate=lr_schedule,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=5e-2,\n",
    "        tensorboard_log=\"./ppo_makruk_tb/\"\n",
    "    )\n",
    "\n",
    "# — Train —\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    callback=[checkpoint_cb, eval_cb, entropy_cb] + extra_cbs\n",
    ")\n",
    "\n",
    "# — Save —\n",
    "model.save(\"ppo_makruk_self_pvp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8ae68",
   "metadata": {},
   "source": [
    "### New Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf47539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Headless vectorized training\n",
      "▶ Re-initializing on new action-space from ppo_makruk_self_pvp.zip\n",
      "Using mps device\n",
      "✅ Loaded old weights except action_net → new head is randomly initialized\n",
      "Logging to ./ppo_makruk_tb/\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | -9.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 255      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -9.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008059307 |\n",
      "|    clip_fraction        | 0.0726      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | -0.011      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.385       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# %% Training cell (with GUI callback that disables itself on window‐close)\n",
    "\n",
    "USE_GUI       = False\n",
    "TOTAL_STEPS   = 300_000\n",
    "ENGINE_DEPTH  = 1\n",
    "\n",
    "import os, time, torch\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from makruk_env import FairyStockfishMakruk\n",
    "\n",
    "# ─── build lr schedule + entropy annealer ────────────────────────────────────\n",
    "lr_schedule = get_schedule_fn(1e-4)\n",
    "class EntropyAnnealer(BaseCallback):\n",
    "    def __init__(self, switch_step, new_ent, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.switch_step = switch_step\n",
    "        self.new_ent     = new_ent\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps == self.switch_step:\n",
    "            print(f\"[EntropyAnnealer] Switching ent_coef → {self.new_ent}\")\n",
    "            self.model.ent_coef = self.new_ent\n",
    "        return True\n",
    "\n",
    "entropy_cb = EntropyAnnealer(TOTAL_STEPS//2, 1e-3)\n",
    "\n",
    "# ─── env fns ─────────────────────────────────────────────────────────────────\n",
    "def make_headless_env(rank):\n",
    "    e = FairyStockfishMakruk(\n",
    "        path=\"./engine/fairy-stockfish-arm\",\n",
    "        max_depth=ENGINE_DEPTH,\n",
    "        play_mode=\"selfplay\",\n",
    "        engine_timeout=2.0,\n",
    "        render_mode=None\n",
    "    )\n",
    "    e = Monitor(e)\n",
    "    return ActionMasker(e, lambda ev: ev.env.get_legal_moves_mask())\n",
    "\n",
    "if USE_GUI:\n",
    "    # … your GUI env code …\n",
    "    env = make_headless_env(0)  # placeholder\n",
    "    extra_cbs = []  # + RenderCallback()\n",
    "else:\n",
    "    print(\">>> Headless vectorized training\")\n",
    "    n_envs = 8\n",
    "    vec   = DummyVecEnv([lambda i=i: make_headless_env(i) for i in range(n_envs)])\n",
    "    env   = VecMonitor(vec)\n",
    "    extra_cbs = []\n",
    "\n",
    "checkpoint_cb = CheckpointCallback(\n",
    "    save_freq=50_000, save_path=\"./checkpoints/\", name_prefix=\"ppo_makruk\"\n",
    ")\n",
    "eval_cb = MaskableEvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    log_path=\"./eval_logs/\",\n",
    "    eval_freq=50_000,\n",
    "    n_eval_episodes=8,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# ─── Load old + reinit action‐head ───────────────────────────────────────────\n",
    "# — Load existing model or create new —\n",
    "# model_path = \"./best_model/best_model.zip\"\n",
    "# model_path = \"ppo_makruk_notebook.zip\"\n",
    "# model_path = \"ppo_makruk_pvp.zip\"\n",
    "model_path = \"ppo_makruk_self_pvp.zip\"\n",
    "\n",
    "\n",
    "if os.path.isfile(model_path):\n",
    "    print(f\"▶ Re-initializing on new action-space from {model_path}\")\n",
    "\n",
    "    # 1) load old model without env to grab its weights\n",
    "    old = MaskablePPO.load(model_path, env=None, device=\"cpu\")\n",
    "    old_state = old.policy.state_dict()\n",
    "\n",
    "    # 2) filter out the old actor head (\"action_net\") keys\n",
    "    filtered = {k: v for k, v in old_state.items() if not k.startswith(\"action_net\")}\n",
    "\n",
    "    # 3) instantiate brand-new model on new env (4 076 actions)\n",
    "    model = MaskablePPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        device=\"mps\",\n",
    "        verbose=1,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        learning_rate=lr_schedule,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=5e-2,\n",
    "        tensorboard_log=\"./ppo_makruk_tb/\"\n",
    "    )\n",
    "\n",
    "    # 4) load everything _but_ the action head\n",
    "    model.policy.load_state_dict(filtered, strict=False)\n",
    "    print(\"✅ Loaded old weights except action_net → new head is randomly initialized\")\n",
    "\n",
    "    # 5) tweak hyperparams & logger\n",
    "    model.ent_coef    = 5e-2\n",
    "    model.lr_schedule = lr_schedule\n",
    "    for pg in model.policy.optimizer.param_groups:\n",
    "        pg[\"lr\"] = lr_schedule(model.num_timesteps)\n",
    "    new_logger = configure(\"./ppo_makruk_tb/\", [\"stdout\",\"tensorboard\"])\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "else:\n",
    "    print(\"▶ Starting fresh model\")\n",
    "    model = MaskablePPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        device=\"mps\",\n",
    "        verbose=1,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        learning_rate=lr_schedule,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=5e-2,\n",
    "        tensorboard_log=\"./ppo_makruk_tb/\"\n",
    "    )\n",
    "\n",
    "# ─── Train & save ────────────────────────────────────────────────────────────\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    callback=[checkpoint_cb, eval_cb, entropy_cb] + extra_cbs\n",
    ")\n",
    "model.save(\"ppo_makruk_self_pvp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268dadb9",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f85a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:11:52.587 python[48292:18656827] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-05-04 13:11:52.587 python[48292:18656827] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over: checkmate, reward=-1.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %% Testing cell (GUI only, with explicit mask)\n",
    "\n",
    "import time\n",
    "import pygame\n",
    "from sb3_contrib import MaskablePPO\n",
    "from makruk_env import FairyStockfishMakruk\n",
    "\n",
    "# 1) Load your trained model\n",
    "# model = MaskablePPO.load(\"ppo_makruk_notebook.zip\", device=\"mps\")\n",
    "model = MaskablePPO.load(\"./best_model/best_model.zip\", device=\"mps\")\n",
    "\n",
    "# 2) Create the GUI env\n",
    "env = FairyStockfishMakruk(\n",
    "    path=\"./engine/fairy-stockfish-arm\",\n",
    "    max_depth=1,\n",
    "    play_mode=\"selfplay\",\n",
    "    engine_timeout=2.0,\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "# 3) Reset & initial draw\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "time.sleep(1.0)\n",
    "\n",
    "# 4) Play one self-play game\n",
    "done = False\n",
    "while not done:\n",
    "    # Keep window alive\n",
    "    for e in pygame.event.get():\n",
    "        if e.type == pygame.QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    # 4a) Compute the mask of legal moves\n",
    "    mask = env.get_legal_moves_mask()\n",
    "\n",
    "    # 4b) Tell the model to only pick from those legal moves  \n",
    "    #     Note: the keyword is **action_masks=**, not mask=\n",
    "    action, _ = model.predict(obs, action_masks=mask, deterministic=True)\n",
    "    \n",
    "    # 5) Step and render\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# 6) Done\n",
    "print(f\"Game over: {info.get('end_reason')}, reward={reward}\")\n",
    "env.close()\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e4f96",
   "metadata": {},
   "source": [
    "### Tensor Board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the very first cell\n",
    "%load_ext tensorboard\n",
    "\n",
    "# in any cell\n",
    "%tensorboard --logdir ./ppo_makruk_tb --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
