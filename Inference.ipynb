{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ec619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selfplay_train.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from makruk_env import FairyStockfishMakruk\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "class SelfPlayMakruk(FairyStockfishMakruk):\n",
    "    \"\"\"\n",
    "    A Makruk Env where the 'opponent' is a fixed PPO model.\n",
    "    We update self (the 'current' PPO) but keep self.opponent frozen.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        opponent_path: str,\n",
    "        device: str = \"cpu\",\n",
    "        **kwargs  # everything FairyStockfishMakruk.__init__ accepts\n",
    "    ):\n",
    "        # 1) Init base env\n",
    "        super().__init__(**kwargs)\n",
    "        # 2) Load frozen opponent\n",
    "        try:\n",
    "            # try maskable first\n",
    "            self.opponent = MaskablePPO.load(opponent_path, device=device)\n",
    "            self._is_maskable = True\n",
    "        except ValueError:\n",
    "            # fallback to vanilla SB3 PPO\n",
    "            self.opponent = PPO.load(opponent_path, device=device)\n",
    "            self._is_maskable = False\n",
    "        # 3) Force self-play mode\n",
    "        self.play_mode = \"selfplay\"\n",
    "\n",
    "    def step(self, action):\n",
    "        # 1) Agent plays\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        if done:\n",
    "            return obs, reward, done, truncated, info\n",
    "\n",
    "        # 2) Opponent plays\n",
    "        mask = self.get_legal_moves_mask()\n",
    "        opp_act, _ = self.opponent.predict(obs, action_masks=mask, deterministic=True)\n",
    "        obs, opp_reward, done, truncated, info = super().step(opp_act)\n",
    "\n",
    "        # 3) Invert opponent’s reward\n",
    "        return obs, reward - opp_reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a70542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Paths to your checkpoints\n",
    "CURRENT_MODEL_PATH = \"./ppo_makruk_pvp.zip\"\n",
    "# CURRENT_MODEL_PATH = \"./ppo_makruk_self_pvp.zip\"\n",
    "# CURRENT_MODEL_PATH = \"./ppo_makruk_notebook.zip\"\n",
    "# BEST_MODEL_PATH    = \"./ppo_makruk_pvp.zip\"\n",
    "# BEST_MODEL_PATH    = \"./best_model/best_model.zip\"\n",
    "# BEST_MODEL_PATH    = \"./ppo_makruk_self_pvp.zip\"\n",
    "BEST_MODEL_PATH = \"./ppo_imitation_raw_policy.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d5f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import MaskablePPO\n",
    "model = MaskablePPO.load(CURRENT_MODEL_PATH, device=\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e9e9b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harryphoebus/miniconda3/envs/deep_rl/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseAlgorithm.predict() got an unexpected keyword argument 'action_masks'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m     mask = eval_env.get_legal_moves_mask()\n\u001b[32m     20\u001b[39m     action, _ = model.predict(obs, action_masks=mask, deterministic=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     obs, reward, done, _, info = \u001b[43meval_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# reward > 0  → our agent delivered mate (win)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# reward < 0  → opponent delivered mate (loss)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reward > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mSelfPlayMakruk.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# 2) Opponent plays\u001b[39;00m\n\u001b[32m     50\u001b[39m mask = \u001b[38;5;28mself\u001b[39m.get_legal_moves_mask()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m opp_act, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopponent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m obs, opp_reward, done, truncated, info = \u001b[38;5;28msuper\u001b[39m().step(opp_act)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# 3) Invert opponent’s reward\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: BaseAlgorithm.predict() got an unexpected keyword argument 'action_masks'"
     ]
    }
   ],
   "source": [
    "# 7) Quick headless self-play evaluation vs. the frozen “best” model\n",
    "eval_env = SelfPlayMakruk(\n",
    "    opponent_path=BEST_MODEL_PATH,\n",
    "    device=\"mps\",\n",
    "    path=\"./engine/fairy-stockfish-arm\",\n",
    "    max_depth=1,            # opponent difficulty\n",
    "    engine_timeout=2.0,\n",
    "    render_mode=None        # headless\n",
    ")\n",
    "\n",
    "# 3) Run N episodes, tallying wins vs. losses\n",
    "n_eval = 100\n",
    "wins, losses = 0, 0\n",
    "\n",
    "for _ in range(n_eval):\n",
    "    obs, info = eval_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        mask = eval_env.get_legal_moves_mask()\n",
    "        action, _ = model.predict(obs, action_masks=mask, deterministic=True)\n",
    "        obs, reward, done, _, info = eval_env.step(action)\n",
    "\n",
    "    # reward > 0  → our agent delivered mate (win)\n",
    "    # reward < 0  → opponent delivered mate (loss)\n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "    elif reward < 0:\n",
    "        losses += 1\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Against frozen best over {n_eval} games → wins={wins}, losses={losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e770a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "class SelfPlayMakruk(FairyStockfishMakruk):\n",
    "    def __init__(self, opponent_path: str, device: str = \"cpu\", **kwargs):\n",
    "        # 1) Init base env\n",
    "        super().__init__(**kwargs)\n",
    "        # 2) Load frozen opponent with the right loader\n",
    "        try:\n",
    "            # try maskable first\n",
    "            self.opponent = MaskablePPO.load(opponent_path, device=device)\n",
    "            self._is_maskable = True\n",
    "        except ValueError:\n",
    "            # fallback to vanilla SB3 PPO\n",
    "            self.opponent = PPO.load(opponent_path, device=device)\n",
    "            self._is_maskable = False\n",
    "\n",
    "        # 3) Force self-play mode\n",
    "        self.play_mode = \"selfplay\"\n",
    "\n",
    "    def get_best_move(self, depth=None):\n",
    "        # 1) Observation + mask\n",
    "        obs  = self.get_fen_tensor()\n",
    "        mask = self.get_legal_moves_mask()\n",
    "\n",
    "        if self._is_maskable:\n",
    "            # MaskablePPO → supply action_masks\n",
    "            a2, _ = self.opponent.predict(obs, action_masks=mask, deterministic=True)\n",
    "        else:\n",
    "            # vanilla PPO → ignore masks\n",
    "            a2, _ = self.opponent.predict(obs, deterministic=True)\n",
    "\n",
    "        return self.uci_moves[a2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7849e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     a1, _ = \u001b[43magent1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     obs, reward, done, _, _ = env.step(a1)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reward > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:307\u001b[39m, in \u001b[36mMaskablePPO.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic, action_masks)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    288\u001b[39m     observation: Union[np.ndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np.ndarray]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    292\u001b[39m     action_masks: Optional[np.ndarray] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    293\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np.ndarray, ...]]]:\n\u001b[32m    294\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[33;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[33;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/sb3_contrib/common/maskable/policies.py:305\u001b[39m, in \u001b[36mMaskableActorCriticPolicy.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic, action_masks)\u001b[39m\n\u001b[32m    302\u001b[39m obs_tensor, vectorized_env = \u001b[38;5;28mself\u001b[39m.obs_to_tensor(observation)\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     actions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m     \u001b[38;5;66;03m# Convert to numpy\u001b[39;00m\n\u001b[32m    307\u001b[39m     actions = actions.cpu().numpy()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/sb3_contrib/common/maskable/policies.py:266\u001b[39m, in \u001b[36mMaskableActorCriticPolicy._predict\u001b[39m\u001b[34m(self, observation, deterministic, action_masks)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    254\u001b[39m     observation: PyTorchObs,\n\u001b[32m    255\u001b[39m     deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    256\u001b[39m     action_masks: Optional[np.ndarray] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    257\u001b[39m ) -> th.Tensor:\n\u001b[32m    258\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[32m    260\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m.get_actions(deterministic=deterministic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/sb3_contrib/common/maskable/policies.py:364\u001b[39m, in \u001b[36mMaskableActorCriticPolicy.get_distribution\u001b[39m\u001b[34m(self, obs, action_masks)\u001b[39m\n\u001b[32m    362\u001b[39m features = \u001b[38;5;28msuper\u001b[39m().extract_features(obs, \u001b[38;5;28mself\u001b[39m.pi_features_extractor)\n\u001b[32m    363\u001b[39m latent_pi = \u001b[38;5;28mself\u001b[39m.mlp_extractor.forward_actor(features)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m distribution = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action_masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    366\u001b[39m     distribution.apply_masking(action_masks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/sb3_contrib/common/maskable/policies.py:250\u001b[39m, in \u001b[36mMaskableActorCriticPolicy._get_action_dist_from_latent\u001b[39m\u001b[34m(self, latent_pi)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[33;03mRetrieve action distribution given the latent codes.\u001b[39;00m\n\u001b[32m    245\u001b[39m \n\u001b[32m    246\u001b[39m \u001b[33;03m:param latent_pi: Latent code for the actor\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m:return: Action distribution\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m action_logits = \u001b[38;5;28mself\u001b[39m.action_net(latent_pi)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/sb3_contrib/common/maskable/distributions.py:136\u001b[39m, in \u001b[36mMaskableCategoricalDistribution.proba_distribution\u001b[39m\u001b[34m(self, action_logits)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproba_distribution\u001b[39m(\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfMaskableCategoricalDistribution, action_logits: th.Tensor\n\u001b[32m    133\u001b[39m ) -> SelfMaskableCategoricalDistribution:\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# Restructure shape to align with logits\u001b[39;00m\n\u001b[32m    135\u001b[39m     reshaped_logits = action_logits.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.action_dim)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28mself\u001b[39m.distribution = \u001b[43mMaskableCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreshaped_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/sb3_contrib/common/maskable/distributions.py:45\u001b[39m, in \u001b[36mMaskableCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args, masks)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(probs, logits, validate_args)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mself\u001b[39m._original_logits = \u001b[38;5;28mself\u001b[39m.logits\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/sb3_contrib/common/maskable/distributions.py:68\u001b[39m, in \u001b[36mMaskableCategorical.apply_masking\u001b[39m\u001b[34m(self, masks)\u001b[39m\n\u001b[32m     65\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m._original_logits\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Reinitialize with updated logits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# self.probs may already be cached, so we must force an update\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m.probs = logits_to_probs(\u001b[38;5;28mself\u001b[39m.logits)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/torch/distributions/categorical.py:72\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n\u001b[32m     69\u001b[39m batch_shape = (\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     71\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_rl/lib/python3.11/site-packages/torch/distributions/distribution.py:70\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     68\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[32m     69\u001b[39m         valid = constraint.check(value)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m     71\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     72\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m             )\n\u001b[32m     78\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent1 = MaskablePPO.load(\"ppo_makruk_self_pvp.zip\", device=\"mps\")\n",
    "agent2_path = \"./ppo_imitation_raw_policy.zip\"  # could be Maskable or not\n",
    "\n",
    "env = SelfPlayMakruk(\n",
    "    opponent_path=agent2_path,\n",
    "    device=\"cpu\",\n",
    "    path=\"./engine/fairy-stockfish-arm\",\n",
    "    max_depth=1,\n",
    "    engine_timeout=2.0,\n",
    "    render_mode=None\n",
    ")\n",
    "\n",
    "wins = losses = 0\n",
    "for ep in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        a1, _ = agent1.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _, _ = env.step(a1)\n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "    elif reward < 0:\n",
    "        losses += 1\n",
    "\n",
    "print(f\"Agent1 wins={wins}, Agent2 wins={losses}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
